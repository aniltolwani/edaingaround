{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb3e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import hashlib\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601617f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exact_dup_dataset():\n",
    "    \"\"\"Generate dataset with exact duplicates for MD5 practice\"\"\"\n",
    "    base_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Python is a great programming language\",\n",
    "        \"Machine learning is transforming industries\",\n",
    "        \"Data science requires statistics knowledge\",\n",
    "        \"Natural language processing is fascinating\"\n",
    "    ]\n",
    "    \n",
    "    dataset = []\n",
    "    for i, text in enumerate(base_texts * 3):  # Each appears 3 times\n",
    "        # Add some with different IDs but same text\n",
    "        dataset.append({\n",
    "            'id': f'doc_{len(dataset)}',\n",
    "            'text': text,\n",
    "            'source': random.choice(['web', 'book', 'paper'])\n",
    "        })\n",
    "    \n",
    "    # Add some unique ones\n",
    "    for i in range(5):\n",
    "        dataset.append({\n",
    "            'id': f'doc_{len(dataset)}',\n",
    "            'text': f\"Unique text number {i} with random content\",\n",
    "            'source': 'generated'\n",
    "        })\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def generate_near_dup_dataset():\n",
    "    \"\"\"Generate dataset with near-duplicates for Jaccard practice\"\"\"\n",
    "    base = \"The quick brown fox jumps over the lazy dog\"\n",
    "    \n",
    "    dataset = [\n",
    "        {\"id\": 0, \"text\": base},\n",
    "        {\"id\": 1, \"text\": \"The quick brown fox jumps over a lazy dog\"},  # 90% similar\n",
    "        {\"id\": 2, \"text\": \"A quick brown fox jumped over the lazy dog\"},  # 80% similar\n",
    "        {\"id\": 3, \"text\": \"The fast brown fox jumps over the lazy cat\"},  # 70% similar\n",
    "        {\"id\": 4, \"text\": \"The quick red fox runs over the sleepy dog\"},  # 50% similar\n",
    "        {\"id\": 5, \"text\": \"Completely different text about programming\"},  # 0% similar\n",
    "        {\"id\": 6, \"text\": \"The quick brown fox jumps over the lazy dog!\"},  # Punctuation diff\n",
    "        {\"id\": 7, \"text\": \"THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\"},  # Case diff\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "def generate_spam_dataset():\n",
    "    \"\"\"Generate dataset with repetitive spam for pattern detection\"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Type 1: Word repetition spam\n",
    "    spam_words = ['buy', 'now', 'click', 'free', 'offer']\n",
    "    for i in range(3):\n",
    "        text = ' '.join([random.choice(spam_words) for _ in range(50)])\n",
    "        dataset.append({\"id\": len(dataset), \"text\": text, \"label\": \"spam\"})\n",
    "    \n",
    "    # Type 2: Phrase repetition spam  \n",
    "    phrase = \"amazing deal click here\"\n",
    "    dataset.append({\"id\": len(dataset), \"text\": ' '.join([phrase] * 10), \"label\": \"spam\"})\n",
    "    \n",
    "    # Type 3: Mixed repetition\n",
    "    template = \"Call now {} best price {} limited time {}\"\n",
    "    nums = ['555-1234', '555-5678', '555-9999']\n",
    "    for num in nums:\n",
    "        text = template.format(num, num, num) * 5\n",
    "        dataset.append({\"id\": len(dataset), \"text\": text, \"label\": \"spam\"})\n",
    "    \n",
    "    # Add legitimate texts\n",
    "    legit_texts = [\n",
    "        \"Machine learning models require careful validation to avoid overfitting on training data\",\n",
    "        \"The conference will feature speakers from industry and academia discussing AI ethics\",\n",
    "        \"Python's simplicity makes it an excellent choice for data science applications\",\n",
    "        \"Understanding statistics is fundamental to interpreting machine learning results correctly\",\n",
    "        \"Cloud computing has revolutionized how we deploy and scale applications\"\n",
    "    ]\n",
    "    \n",
    "    for text in legit_texts:\n",
    "        dataset.append({\"id\": len(dataset), \"text\": text, \"label\": \"ham\"})\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def generate_shingle_dataset():\n",
    "    \"\"\"Generate dataset for shingle/MinHash practice\"\"\"\n",
    "    # Similar news articles with overlapping content\n",
    "    articles = [\n",
    "        \"\"\"Apple announced record quarterly earnings driven by strong iPhone sales.\n",
    "        The company reported revenue of $90 billion, exceeding analyst expectations.\n",
    "        CEO Tim Cook attributed success to innovation and customer loyalty.\"\"\",\n",
    "        \n",
    "        \"\"\"Apple reported record earnings this quarter with strong iPhone performance.\n",
    "        Revenue reached $90 billion, surpassing Wall Street expectations significantly.\n",
    "        Tim Cook cited innovation as key driver of the company's success.\"\"\",\n",
    "        \n",
    "        \"\"\"Tech giant Apple posted impressive quarterly results led by iPhone sales.\n",
    "        The firm announced $90 billion in revenue, beating market forecasts.\n",
    "        Chief executive Cook highlighted product innovation and loyal customers.\"\"\",\n",
    "        \n",
    "        \"\"\"Microsoft announced cloud growth driving quarterly earnings higher.\n",
    "        Azure revenue increased 40% as enterprises accelerate digital transformation.\n",
    "        CEO Satya Nadella emphasized AI integration across product lines.\"\"\",\n",
    "        \n",
    "        \"\"\"Amazon Web Services continues dominating cloud infrastructure market.\n",
    "        AWS revenue grew 35% year-over-year reaching new milestone.\n",
    "        The company plans significant AI and machine learning investments.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    dataset = [{\"id\": i, \"text\": article.replace('\\n', ' ').strip()} \n",
    "               for i, article in enumerate(articles)]\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a9b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROBLEM 1: EXACT DEDUPLICATION WITH MD5\n",
      "============================================================\n",
      "\n",
      "Dataset has 20 documents, some are exact duplicates.\n",
      "Task: Find and remove exact duplicates using MD5 hashing\n",
      "Expected: 5 unique documents + 5 generated = 10 unique total\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROBLEM 1: EXACT DEDUPLICATION WITH MD5\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDataset has 20 documents, some are exact duplicates.\")\n",
    "print(\"Task: Find and remove exact duplicates using MD5 hashing\")\n",
    "print(\"Expected: 5 unique documents + 5 generated = 10 unique total\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa9b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 20 documents\n",
      "First 3 documents:\n",
      "  ID: doc_12, Text: Machine learning is transforming industries...\n",
      "  ID: doc_1, Text: Python is a great programming language...\n",
      "  ID: doc_14, Text: Natural language processing is fascinating...\n"
     ]
    }
   ],
   "source": [
    "exact_data = generate_exact_dup_dataset()\n",
    "print(f\"Dataset size: {len(exact_data)} documents\")\n",
    "print(\"First 3 documents:\")\n",
    "for doc in exact_data[:3]:\n",
    "    print(f\"  ID: {doc['id']}, Text: {doc['text'][:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0e9ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YOUR TASK: Write a function to find exact duplicates using MD5\n",
      "Return: dict with keys: 'unique_count', 'duplicate_ids', 'hash_collisions'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unique_count': 10,\n",
       " 'duplicate_ids': ['doc_11',\n",
       "  'doc_7',\n",
       "  'doc_6',\n",
       "  'doc_4',\n",
       "  'doc_0',\n",
       "  'doc_5',\n",
       "  'doc_2',\n",
       "  'doc_9',\n",
       "  'doc_13',\n",
       "  'doc_8'],\n",
       " 'hash_collisions': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\nYOUR TASK: Write a function to find exact duplicates using MD5\")\n",
    "print(\"Return: dict with keys: 'unique_count', 'duplicate_ids', 'hash_collisions'\")\n",
    "\n",
    "from hashlib import md5\n",
    "\n",
    "res = {\n",
    "    \"unique_count\": 0,\n",
    "    \"duplicate_ids\": [],\n",
    "    \"hash_collisions\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "digests = set()\n",
    "for doc in exact_data:\n",
    "    hashed_text = md5(doc['text'].encode()).digest()\n",
    "    if hashed_text in digests:\n",
    "        res[\"duplicate_ids\"] = res[\"duplicate_ids\"] + [doc['id']]\n",
    "    else:\n",
    "        res['unique_count'] += 1\n",
    "        # this seems a bit pointless... if we have a set we could use that directly\n",
    "        digests.add(hashed_text)\n",
    "    # what is a hash collision? if they are equal doesnt that imply they are just the same text?\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea9ccf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROBLEM 2: NEAR-DUPLICATE DETECTION WITH JACCARD\n",
      "============================================================\n",
      "\n",
      "Dataset has texts with varying similarity to the first document\n",
      "Task: Find all documents with Jaccard similarity > 0.7 to doc 0\n",
      "Expected: IDs 0, 1, 2, 3, 6, 7 (after normalization)\n",
      "\n",
      "  ID: 0, Text: The quick brown fox jumps over the lazy dog\n",
      "  ID: 1, Text: The quick brown fox jumps over a lazy dog\n",
      "  ID: 2, Text: A quick brown fox jumped over the lazy dog\n",
      "  ID: 3, Text: The fast brown fox jumps over the lazy cat\n",
      "  ID: 4, Text: The quick red fox runs over the sleepy dog\n",
      "  ID: 5, Text: Completely different text about programming\n",
      "  ID: 6, Text: The quick brown fox jumps over the lazy dog!\n",
      "  ID: 7, Text: THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n",
      "\n",
      "YOUR TASK: Write jaccard_similarity(text1, text2) function\n",
      "Normalize: lowercase, remove punctuation\n",
      "Return: list of IDs with similarity > 0.7 to document 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROBLEM 2: NEAR-DUPLICATE DETECTION WITH JACCARD\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDataset has texts with varying similarity to the first document\")\n",
    "print(\"Task: Find all documents with Jaccard similarity > 0.7 to doc 0\")\n",
    "print(\"Expected: IDs 0, 1, 2, 3, 6, 7 (after normalization)\\n\")\n",
    "\n",
    "near_data = generate_near_dup_dataset()\n",
    "for doc in near_data:\n",
    "    print(f\"  ID: {doc['id']}, Text: {doc['text']}\")\n",
    "\n",
    "print(\"\\nYOUR TASK: Write jaccard_similarity(text1, text2) function\")\n",
    "print(\"Normalize: lowercase, remove punctuation\")\n",
    "print(\"Return: list of IDs with similarity > 0.7 to document 0\")\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    # lowercase + remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # split removes duplicate whitespace too \n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    # jaccard is intersection / union\n",
    "    t1_set = set(normalize(text1))\n",
    "    t2_set = set(normalize(text2))\n",
    "    if not t1_set and not t2_set:\n",
    "        return 1\n",
    "    elif not t1_set or not t2_set:\n",
    "        return 0\n",
    "    return len(t1_set & t2_set) / len(t1_set | t2_set)\n",
    "jaccard_similarity(near_data[0]['text'], near_data[3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b22954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROBLEM 3: SPAM DETECTION WITH REPETITION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Dataset has spam and legitimate messages\n",
      "Task: Detect spam using word/phrase repetition patterns\n",
      "Expected: 7 spam messages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROBLEM 3: SPAM DETECTION WITH REPETITION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDataset has spam and legitimate messages\")\n",
    "print(\"Task: Detect spam using word/phrase repetition patterns\")\n",
    "print(\"Expected: 7 spam messages\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be47db1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'a', 'b', 'c'}, {'a', 'b', 'c'})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a', 'b', 'c'}\n",
    "b = {'b', 'a', 'c', 'd'}\n",
    "a & b, a.intersection(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2e967bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12 documents\n",
      "Sample spam: amazing deal click here amazing deal click here amazing deal click here amazing deal click here amaz...\n",
      "Sample ham: The conference will feature speakers from industry and academia discussing AI ethics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spam_data = generate_spam_dataset()\n",
    "print(f\"Dataset size: {len(spam_data)} documents\")\n",
    "print(\"Sample spam:\", spam_data[0]['text'][:100] + \"...\")\n",
    "print(\"Sample ham:\", [d for d in spam_data if d['label'] == 'ham'][0]['text'][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "02dcc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YOUR TASK: Write is_spam(text) using repetition metrics\n",
      "Consider: unique word ratio, bigram repetition, phrase patterns\n",
      "Return: accuracy on the labeled dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.14285714285714285)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"\\nYOUR TASK: Write is_spam(text) using repetition metrics\")\n",
    "print(\"Consider: unique word ratio, bigram repetition, phrase patterns\")\n",
    "print(\"Return: accuracy on the labeled dataset\")\n",
    "\n",
    "\n",
    "# phrase patterns\n",
    "\n",
    "text = \"the quick brown fox jumped over quick brown\"\n",
    "def is_spam(text):\n",
    "    tokens = text.split()\n",
    "    # look at bigram repetition\n",
    "    bigrams = list(zip(tokens, tokens[1:]))\n",
    "    # look at repetition\n",
    "    repeat = (len(bigrams) - len(set(bigrams))) / len(bigrams)\n",
    "    counter = Counter(tokens)\n",
    "    most_common = counter.most_common(1)[0][1] / len(tokens)\n",
    "    return most_common, repeat\n",
    "\n",
    "is_spam(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783a3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
